{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **MoroccoAI Data Challenge (Edition 001)**\n",
        "\n",
        "This notebook walks through The prcoccess of detecting plates from images using our 2 Fast-RCNN models that were trained on Plate Detection and Moroccan Plate Charachter Detection, and the post-processing that followed the predection.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Overview**\n",
        "\n",
        "In Morocco, the number of registered vehicles doubled between 2000 and 2019. In 2019, a few months before lockdowns due to the Coronavirus Pandemic, 8 road fatalities were recorded per 10 000 registered vehicles. This rate is extremely high when compared with other IRTAD countries. The National Road Safety Agency (NARSA) established the road safety strategy 2017-26 with the main target to reduce the number of road deaths by 50% between 2015 and 2026 [1].Is crucial for law enforcement and authorities in order to assure the safety of the roads and to check the registration and the licence of the vehicles.\n",
        "Therefore the aim to automate this task is very beneficial.\n",
        "\n",
        "**This Jupyter Notebook only loads the trained Checkpoints, You can find Training Notebook in the next link.**\n",
        "\n",
        "üí° Recommendation: [The Jupyter Notebook were we trained our models to detect Plates from pictures at first stage then detect Characters from Plates](https://colab.research.google.com/drive/1Niz1AVejRSm8UKFolP7DWla8WRpq6JwD?usp=sharing).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Dataset**\n",
        "\n",
        "The dataset is 654 jpg pictures of the front or back of vehicles showing the license plate. They are of different sizes and are mostly cars. The plate license follows Moroccan standard.\n",
        "\n",
        "For each plate corresponds a string (series of numbers and latin characters) labeled manually. The plate strings could contain a series of numbers and latin letters of different length. Because letters in Morocco license plate standard are Arabic letters, we will consider the following transliteration: a <=> ÿ£, b <=> ÿ®, j <=> ÿ¨ (jamaa), d <=> ÿØ , h <=> Ÿá , waw <=> Ÿà, w <=> w (newly licensed cars), p <=> ÿ¥ (police), fx <=> ŸÇ ÿ≥ (auxiliary forces), far <=> ŸÇ ŸÖ ŸÖ (royal army forces), m <=>ÿßŸÑŸÖÿ∫ÿ±ÿ®, m <=>M. For example:\n",
        "\n",
        "the string ‚Äú123ÿ®45‚Äù have to be converted to ‚Äú12345b‚Äù,<br>\n",
        "the string ‚Äú123Ÿà4567‚Äù to ‚Äú1234567waw‚Äù,<br>\n",
        "the string ‚Äú12Ÿà4567‚Äù to ‚Äú1234567waw‚Äù,<br>\n",
        "the string ‚Äú1234567ww‚Äù to ‚Äú1234567ww‚Äù, (remain the same)<br>\n",
        "the string ‚Äú1234567far‚Äù to ‚Äú1234567ŸÇ ŸÖ ŸÖ‚Äù,<br>\n",
        "the string ‚Äú1234567m‚Äù to ‚Äú1234567ÿßŸÑŸÖÿ∫ÿ±ÿ®\", etc.\n",
        "<br>\n",
        "\n",
        "We offer the plate strings of 450 images (training set). The remaining 204 unlabeled images will be the test set. The participants are asked to provide the plate strings in the test set.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Our Approach & Models**\n",
        "\n",
        "Our approach was to use Object Detection to detect plate characters from images. We have chosen to build two models separately instead of using libraries directly like easyOCR or Tesseract due to its weaknesses in handling the variance in the shapes of Moroccan License plates.\n",
        "The first model was trained to detect the licence plate to be then cropped from the original image, which will be then passed into the second model that was trained to detect the characters. \n",
        "\n",
        "This notebook will be showing a code example on pretrained faster-rcnn model for both Object detection tasks,  using library called detectron2 developed by FaceBook AI Research Laboratory (FAIR) based on Pytorch.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Detectron2**\n",
        "\n",
        "#### ![Detectron2 Logo](https://raw.githubusercontent.com/facebookresearch/detectron2/085fda47bc49f2cdd9c05a895580b2b31fcdb6c3/.github/Detectron2-Logo-Horz.svg)\n",
        "\n",
        "Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of Detectron and maskrcnn-benchmark. It supports a number of computer vision research projects and production applications in Facebook.\n",
        "\n",
        "\n",
        "### **Fast-RCNN**\n",
        "\n",
        "#### ![Fast-RCNN Architecture ](https://www.researchgate.net/profile/Akif-Durdu/publication/334987612/figure/fig3/AS:788766109224961@1565067903984/High-level-diagram-of-Faster-R-CNN-16-for-generic-object-detection-2-Inception-v2-The.ppm)\n",
        "\n",
        "This version of the notebook doesn't contain the inference part since it's still on development. Once the inference and deployment part is done this will noteboook will be update.\n",
        "\n",
        "### **About**\n",
        "\n",
        "[MoroccoAI](https://morocco.ai/) MoroccoAI is an initiative led by AI experts in Morocco and abroad to promote AI growth in Morocco across the spectrum.\n",
        "\n",
        "\n",
        "#### ![MoroccoAI Logo](https://morocco.ai/wp-content/uploads/2020/03/MoroccoAI_Logo.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3FERYhqNs8rZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Installing Detecron2<h2>\n",
        "<p>\n",
        "  First thing to do is try to Install detectron2 and Restart the runtime so all installed libraires get loaded.\n",
        "</p>"
      ],
      "metadata": {
        "id": "q2CLH-VLgjbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_5PhcO2Wgk3",
        "outputId": "e7b10e70-91bb-4024-8a4f-422d5781cfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.7/dist-packages (5.1)\n",
            "torch:  1.10 ; cuda:  cu111\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n",
            "Requirement already satisfied: detectron2 in /usr/local/lib/python3.7/dist-packages (0.6+cu111)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.62.3)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.1.1)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.8)\n",
            "Requirement already satisfied: black==21.4b2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (21.4b2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.5.post20211023)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.3)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.1)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.9)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.7.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (3.10.0.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.4.3)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.5.1)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.10.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (2021.11.10)\n",
            "Requirement already satisfied: pathspec<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (4.8)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (5.4.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2) (2.3.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.24)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.42.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml==5.1\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Importing libraries and Packages </h3>"
      ],
      "metadata": {
        "id": "QF7Az8a8hQTi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UmUHpelKWpm-"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import statistics\n",
        "import os, json, cv2, random\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.utils.visualizer import ColorMode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Importing Faster-RCNN model and loading checkpoint for licence Plates detection </h3>\n",
        "Since this notebook is for running models to predict images from testset we will be loading our pretrained model and use \"CPU\" as MODEL.DEVICE"
      ],
      "metadata": {
        "id": "glkbD-L0hgY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cVfeDV-cZVeA"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "#cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "#cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "#cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (licence). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XimWQ_ruZaDu"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(\"./Plate_Detection_Model\", \"model_final.pth\")  # path to the model we just trained\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set a custom testing threshold\n",
        "\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Test folder</h3>\n",
        "<p>in order to generate a csv file of plates countained on the folder,\n",
        "you can pass the test folder's path  to Image_folder vriable\n",
        "</p>"
      ],
      "metadata": {
        "id": "gjpVFRraiOMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9CpcNo5JZx7B"
      },
      "outputs": [],
      "source": [
        "Images_folder = './test'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Extraction Folder</h3>\n",
        "<p>This is the exporting folder for the plate image after extracting it from original images\n",
        "</p>"
      ],
      "metadata": {
        "id": "M1M9x8mLiEaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Extraction_folder = './Plate_detection'"
      ],
      "metadata": {
        "id": "08EEHrJFJ4ZS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "P95ZRKRMbNo2"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(Extraction_folder):\n",
        "    os.makedirs(Extraction_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Plates Extraction</h3>"
      ],
      "metadata": {
        "id": "6kFtpaadkdfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>in order to handle images with multiple licence Plates.\n",
        "We created two function one that extract all plates that exists in Image, and one that extract only the one plate that model predected with highest confidance\n",
        "</p>"
      ],
      "metadata": {
        "id": "fCpQn4TAkq50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UpZlh1gyblvQ"
      },
      "outputs": [],
      "source": [
        "def Plates_Detection_All_Plates_In_Image(Images_folder,Image,Extraction_folder):\n",
        "  im = cv2.imread(os.path.join(Images_folder, Image))\n",
        "  outputs = predictor(im)  \n",
        "  boxes = outputs['instances'].pred_boxes.tensor.cpu().numpy().tolist() \n",
        "  scores = outputs['instances'].scores.numpy().tolist()\n",
        "  if len(scores)>0:\n",
        "      classes = outputs['instances'].pred_classes.to('cpu').tolist()\n",
        "      Plates = { i : boxes[i] for i in range(0, len(scores) ) }\n",
        "      for k,v in Plates.items():\n",
        "          cv2.imwrite(os.path.join(Extraction_folder, Image[:-4]+'_'+str(k)+'.jpg'), im[int(v[1]):int(v[3]), int(v[0]):int(v[2]), :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wsrm_1YHpsbS"
      },
      "outputs": [],
      "source": [
        "def Plates_Detection_Top_Score_Plate_In_image(Images_folder,Image,Extraction_folder):\n",
        "    im = cv2.imread(os.path.join(Images_folder, Image))\n",
        "    outputs = predictor(im)  \n",
        "    boxes = outputs['instances'].pred_boxes.tensor.cpu().numpy().tolist()\n",
        "    scores = outputs['instances'].scores.numpy().tolist()\n",
        "    if len(scores)>0:\n",
        "        classes = outputs['instances'].pred_classes.to('cpu').tolist()\n",
        "        Plates = { i : boxes[i] for i in range(0, len(scores) ) }\n",
        "        Sorted_Plates_by_Score = sorted(Plates.items(), key=lambda e: e[1][0], reverse=True)\n",
        "        Top_Score_Plate = Sorted_Plates_by_Score[0][1]\n",
        "        cv2.imwrite(os.path.join(Extraction_folder, Image), im[int(Top_Score_Plate[1]):int(Top_Score_Plate[3]), int(Top_Score_Plate[0]):int(Top_Score_Plate[2]), :])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>if you want to take in considiration all licence  plates on each image,\n",
        "the parameter all_plates should be set to True, otherwise keep the default variable False\n",
        " </p>"
      ],
      "metadata": {
        "id": "V2r2gz9wlMfr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3lQUpaLXgPoD"
      },
      "outputs": [],
      "source": [
        "def Detect_Plates_From_Images(Images_folder,all_plates=False):\n",
        "  for image in os.listdir(Images_folder):  \n",
        "    if image.lower().endswith(('.png', '.jpg', '.jpeg')) :\n",
        "      if all_plates :\n",
        "        Plates_Detection_All_Plates_In_Image(Images_folder,image,Extraction_folder)\n",
        "      else:\n",
        "        Plates_Detection_Top_Score_Plate_In_image(Images_folder,image,Extraction_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next Code we will be running our Main function which will be predicting and saving our detected plates into the extraction folder."
      ],
      "metadata": {
        "id": "eof3_FNav79L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Detect_Plates_From_Images(Images_folder,all_plates=False)"
      ],
      "metadata": {
        "id": "RBFVl4wAPMF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a528fb36-4083-4661-e380-b8bb6a3b13fb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  max_size = (max_size + (stride - 1)) // stride * stride\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Importing Faster-RCNN model and loading checkpoint for characters detector </h3>"
      ],
      "metadata": {
        "id": "PLcOheZRmCqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "iail5GnJiFcA"
      },
      "outputs": [],
      "source": [
        "cfg_ocr = get_cfg()\n",
        "cfg_ocr.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "cfg_ocr.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "#cfg_ocr.DATALOADER.NUM_WORKERS = 2\n",
        "#cfg_ocr.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "\n",
        "cfg_ocr.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg_ocr.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg_ocr.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg_ocr.MODEL.ROI_HEADS.NUM_CLASSES = 20 # Numbers of charachters that appears in moroccon licence plates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "N6dvJf3Di-tq"
      },
      "outputs": [],
      "source": [
        "cfg_ocr.MODEL.WEIGHTS = os.path.join(\"./Characters_Detection_Model\", \"model_final.pth\")  # path to the model we just trained\n",
        "\n",
        "cfg_ocr.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n",
        "\n",
        "predictor_ocr = DefaultPredictor(cfg_ocr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a dictionary that will take class ID (int) from predictor and return the exact string transformation like it was asked in the challenge "
      ],
      "metadata": {
        "id": "TGZ0ZtSNwWG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "rQk3CQAGjAwC"
      },
      "outputs": [],
      "source": [
        "classlist = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\", \"a\",\"b\",\"h\",\"w\",\"d\",\"p\",\"waw\",\"j\",\"m\",\"m\"]\n",
        "classestolettres = { i : classlist[i] for i in range(0, len(classlist) ) }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Function that return the characters and there bounding boxes(without order)</p>"
      ],
      "metadata": {
        "id": "n0rLHzWkms9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "A8QNwNfgjTqV"
      },
      "outputs": [],
      "source": [
        "def OCR_Predictor(Extraction_folder,Image):\n",
        "    im = cv2.imread(os.path.join(Extraction_folder, Image))\n",
        "    outputs = predictor_ocr(im)  \n",
        "    boxes = outputs['instances'].pred_boxes.tensor.cpu().numpy().tolist()\n",
        "    classes = outputs['instances'].pred_classes.to('cpu').tolist()\n",
        "    dict_Of_predection = { i : [classes[i],boxes[i]] for i in range(0, len(outputs['instances'].pred_classes.to('cpu').tolist()) ) }\n",
        "    return(dict_Of_predection)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is post processing function aim to generate the right sequence of charachters to match the content of a licence plate\n",
        "\n",
        "1.   Split characters based on median of Y_Min of all detected letters boxes, by taking characters  where their Y_Max is smaller than Median_Y_Mins into a string called top_characters, and those who have Y_Max greater than Median_Y_Mins will be in bottom_characters\n",
        "2.   Order characters in top and bottom list from left to right based on the X_Min of the detected Box of each character\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ulrd_4O1x5MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OCR_Plates_Post_Processing(plate,dict_Of_predection):\n",
        "    Plate = [item[1][1][1] for item in list(dict_Of_predection.items())]\n",
        "    if len(Plate)<=0:\n",
        "      ocr_result = {'plate_id':plate[:-4],'plate_string':''}\n",
        "      return(ocr_result)\n",
        "    medYmin = statistics.median(Plate)\n",
        "    toplettres = dict()\n",
        "    bottomlettres = dict()\n",
        "    for k,v in dict_Of_predection.items():\n",
        "      if (v[1][3] <= medYmin ):\n",
        "        toplettres[k] = v\n",
        "      else :\n",
        "        bottomlettres[k] = v\n",
        "    TopRes = sorted(toplettres.items(), key=lambda e: e[1][1][0])\n",
        "    BottomRes = sorted(bottomlettres.items(), key=lambda e: e[1][1][0])\n",
        "    TopPlate = [classestolettres[item[1][0]] for item in TopRes]\n",
        "    BottomPlate = [classestolettres[item[1][0]] for item in BottomRes]\n",
        "    TopPlate = \"\".join(str(x) for x in TopPlate)\n",
        "    BottomPlate = \"\".join(str(x) for x in BottomPlate)\n",
        "    ocr_result = {'plate_id':plate[:-4],'plate_string':BottomPlate+TopPlate}\n",
        "    return(ocr_result)"
      ],
      "metadata": {
        "id": "AJYAE_foZygp"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function is returning a pandas data frame of couples for image names and there prediction"
      ],
      "metadata": {
        "id": "j3oo0G4Bn3oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_OCR_From_Plates(Extraction_folder):\n",
        "  column_names = [\"plate_id\", \"plate_string\"]\n",
        "  submission_result = pd.DataFrame(columns = column_names)\n",
        "  for plate in os.listdir(Extraction_folder):\n",
        "    if plate.lower().endswith(('.png', '.jpg', '.jpeg')) :\n",
        "      dict_Of_predection = OCR_Predictor(Extraction_folder,plate)\n",
        "      ocr_result = OCR_Plates_Post_Processing(plate,dict_Of_predection)\n",
        "      submission_result = submission_result.append(ocr_result, ignore_index=True)\n",
        "      submission_result = submission_result.sort_values(by=['plate_id'], ascending=True)\n",
        "  return(submission_result)"
      ],
      "metadata": {
        "id": "lJmzneTzbSky"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "rsrwe3SMmsM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3404b3ad-34f4-477b-a26d-f8bd4ea7b2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  max_size = (max_size + (stride - 1)) // stride * stride\n"
          ]
        }
      ],
      "source": [
        "submission_result = Get_OCR_From_Plates(Extraction_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "PS-uefSG4Mx8",
        "outputId": "354cd0fb-372e-4f46-cdda-e3cf95e99923"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-838835d1-266c-4738-bfa0-120eecabb2eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plate_id</th>\n",
              "      <th>plate_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>638</td>\n",
              "      <td>88621a40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>639</td>\n",
              "      <td>2905h6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>640</td>\n",
              "      <td>37171waw6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-838835d1-266c-4738-bfa0-120eecabb2eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-838835d1-266c-4738-bfa0-120eecabb2eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-838835d1-266c-4738-bfa0-120eecabb2eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  plate_id plate_string\n",
              "0      638     88621a40\n",
              "2      639       2905h6\n",
              "1      640    37171waw6"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Saving the Data Frame as csv File</p>"
      ],
      "metadata": {
        "id": "MjixXv0Uo0wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_result.to_csv(\"sample_submission.csv\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "Ahlr5oQf1SG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MoroccoAI Data Challenge Submission Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}